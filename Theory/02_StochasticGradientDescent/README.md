# Stochastic Gradient Descent(확률적 경사 하강법)
피드포워드 신경망의 지도 학습법으로 딥러닝에서 자주 쓰이는 방법. <br>

피드포워드 신경망의 학습은 주어진 훈련데이터로부터 계산되는 오차함수 E(w)를 신경망의 파라미터(가중치와 바이어스)w에 대하여 최소화하는 과정 <br>

하지만 오차함수는 일반적인 볼록함수가 아니므로 전역 극소점을 직접구하는 것은 통상적으로 불가능 함 --> 대신 오차함수의 국소 극소점(local minimum) w를 구할수는 있음. <br>

국소 극소점은 보통 여러 개 존재하므로 구한 극소점 중 하나가 우연히 전역 극소점일 가능성은 높지 않음. 하지만 그 국소 극소점 w에서의 오차함수의 값이 충분히 작다면 목적으로 하는 클래스 분류나 회귀 문제를 잘 풀어나갈수 있음 <br>

국소 극소점 하나는 어떤 초기 점을 출발점으로 하고 w(가중치)를 되풀이하여 갱신하는 반복계산을 통해 구할 수 있음 이러한 방법중 하나가 경사 하강법(gradient descent method) 임 <br>

경사 하강법은 현재의 w(가중치)를 음의 기울기 방향으로 조금씩 움직이는 것을 여러번 반복하면 언젠가는 극소점에 도달하게 됨. 이때 **학습률(learning rate)** 를 통해 가중치의 갱신량의 크기를 결정할 수 있는데 학습률의 너무 작으면 가중치 갱신값 변화량이 너무 적어 반복횟수가 많이 필요하고 학습에 걸리는 시간이 길어짐 <br>

학습률을 결정하는 것은 학습에서 매우 중요한 요소로 실제 학습이 잘 진행될지에 대한 열쇠<br><br>


## 확률적 경사 하강법(Stochastic Gradient Descent, SGD)
회귀와 클래스 분류 어느 쪽이든 오차함수(E(w))는 각 샘플 한개에 대해서만 계산되는 오차 En의 합으로 주어짐 <br>

- **배치학습(Batch Learning)**
  - 한번에 모든 훈련 데이터를 학습시키는 방법
- **확률적 경사 하강법(SGD)**
  - 샘플의 일부, 극단적으로는 샘플 하나만을 사용하여 파라미터를 업데이트

배치 형태의 경사 하강법보단 확률적 경사 하강법이 더 많이 쓰임 <br>
확률적 경사 하강법의 장점 <br>
1. 훈련 데이터에 잉여성이 있을 때 계산 효율성이 향상
2. 학습이 빨리 진행됨
3. 반복 계산이 바람직하지 않은(오차함수의 값이 상대적으로 그렇게 작지 않은) 국소 극소점에 갇히는 위험을 줄임 
   1. 배치학습의 경우 최소화하려는 목적함수는 항상 같은 E(w)이므로 바람직하지 않은 국소 극소점에 한번 갇혀 버리면 빠져나오기 힘들지만 확률적 경사하강법의 경우 목적함수 En(w)가 w를 업데이트 할때마다 달라지므로 그런 위험이 상대적으로 덜 함 
4. 파라미터 업데이트 크기가 작은 상태로 학습이 진행되므로 학습의 경과를 좀 더 자세히 보며 진행할 수 있음

<br>

## 미니배치(mini batch)
큰 규모의 신경망 학습은 대규모의 계산 비용이 들기 때문에 수치 계산을 효율적으로 하기 위해 컴퓨터가 갖춘 행렬 계산 자원을 이용해야 함. 이를 위해 샘플 한 개 단위가 아닌 몇 개의 샘플을 하나의 작은 집합으로 묶은 집합 단위로 가중치를 업데이트 함 --> 복수의 샘플을 묶은 작은 집합을 **미니배치(mini batch)** 라 함 <br>

미니 배치의 크기는 대체로 10~100개 전후로 결정하는 경우가 많음. <br>
다클래스 분류의 경우 미니배치 간의 가중치 업데이트 값을 일정하게 하기 위해 미니배치마다 각 클래스의 샘플이 하나 이상 들어가도록 하는 것이 이상적(각 클래스의 출현 빈도가 서로 같을 때는 분류하려는 클래스 수와 같은 크기의 미니배치 생성하는 것이 좋음) <br>

미니배치의 크기를 너무 작게 잡으면 SGD를 완전히 활용하지 못하므로 좋지 않음. 크기를 크게 할수록 미니배치마다 계산되는 기울기가 일정. 가중치의 업데이트 양이 안정되기 때문에 학습률을 크게 잡을 수 있고 그만큼 학습이 빨라짐

<br>

## 일반화 성능과 과적합
딥러닝 학습의 목적은 이미 주어진 훈련 데이터가 아닌 앞으로 주어질 미지의 데이터에 대한 정확한 추정을 가능토록 하는 것. <br>

훈련 데이터에 대한 오차를 **훈련 오차(training error)** , 샘플 모집단에 대한 오차에 대한 기댓값을 **일반화 오차(generalization error)** 라 부르며 구분 
일반화 오차를 작게 하는것이 좋지만 일반화 오차는 통계적 기댓값으로 훈련 오차처럼 계산이 안됨 --> 훈련 데이터와 다른 별도의 샘플 집합을 준비한 후, 이 샘플 집합에 대해서 훈련 오차와 같은 방법으로 계산한 오차를 기준을 삼는 방법 사용, 이를 위해 준비하는 데이터를 **테스트 데이터(test data)** 라 하며 테스트 데이터에 대한 오차를 **테스트 오차(test error)** 라 부름 <br>

일반적으로 훈련 오차는 파라미터의 업데이트를 반복함에 따라 대개 단조적으로 감소. But, 테스트 오차는 학습 초기에는 훈련 오차와 같이 감소하다가 학습 도중 자주 훈련 오차와 많이 달라진 값을 갖게 됨. 심한 경우엔 테스트 오차가 증가하는 경우가 존재 <br>

훈련 오차와 일반화 오차가 동떨어진 값을 갖는 상태가 되는 것을 **과적합(overfitting)** 또는 **과잉적합**, 혹은 **과학습(overlearning)** 이라 부름 <br>

이런 과적합을 일으키지 않고 테스트 오차(~= 일반화 오차)를 보다 작게 하는 것이 신경망 학습의 열쇠

<br>

## 과적합을 완화시키는 방법
### 규제화
과적합이란 학습 시에 오차함수의 값이 작은 국소 극소점에 갇힌 상황. 신경망의 자유도(주로 가중치의 수)가 높을수록 그럴 가능성이 높음 <br>

학습 시에 가중치의 자유도를 제약하는 **규제화(regularization)** 에 의해 과적합 문제를 완화 시킬 수 있음

### 가중치 감쇠
가장 간단한 규제화 방법은 가중치에 어떤 제약을 가하는 것 <br>

오차함수에 가중치의 제곱합을 더한 뒤 이를 최소화하는 방법 이렇게 함으로써 가중치는 자신의 크기에 비례하는 속도로 항상 감쇠하도록 업데이트 이를 **가중치 감쇠(weight decay)** 라 부름 <br>

### 가중치 상한
가중치 값의 상한을 통해 가중치를 제약하는 방법. 각 유닛의 입력 측 결합의 가중치에 대해 그 제곱합의 최댓값을 제약하는 방법. 제곱합의 값이 최댓값을 넘어가게 되는 경우 미리 정한 1보다 작은 상수를 곱하여 최댓값을 넘기지 못하게 만듬. <br>

### 드롭아웃
다층(multi-layer) 신경망의 유닛 중 일부를 확률적으로 선택하여 학습하는 방법 <br>

학습 시에 중간층과 입력 층 각 층의 유닛 중 미리 정해 둔 비율 p만큼 선택하고 선택되지 않은 유닛을 무효화, 원래 존재하지 않았던 것처럼 취급, 선택된 유닛만으로 구성된 가상의 신경망을 최적화. 미니 배치를 적용중이라면 미니배치 단위로 유닛을 다시 선택, 유닛을 선택하는 확률은 각 층마다 다르게 적용해도 상관 없음 <br>

학습이 끝난 뒤 추론 시에는 모든 유닛을 사용해 앞먹임 계산을 함. 단, 드롭아웃에서 무효화된 유닛은 일률적으로 출력을 p배로 함, 이렇게 하는 이유는 추론시의 유닛 수가 학습 시에 비해 1/p배 된 것과 같기 때문에 이를 보상하기 위함 